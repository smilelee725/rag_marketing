{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Beijing Guoan Football Club is a professional soccer team based in Beijing, China. Founded in 1992, the club is one of the most prominent teams in the Chinese Super League (CSL). Beijing Guoan is known for its passionate fan base and has a strong presence in Chinese football.\\n\\nThe team plays its home matches at the Workers' Stadium, which is located in the Chaoyang District of Beijing. The stadium has a significant capacity and is one of the iconic sports venues in the city.\\n\\nBeijing Guoan has enjoyed success in domestic competitions, having won the Chinese Super League title and several domestic cups. The club is also known for its competitive performances in the AFC Champions League, representing Chinese football on the continental stage.\\n\\nThe team's colors are traditionally green and white, and its mascot is a lion, symbolizing strength and courage. Over the years, Beijing Guoan has attracted several high-profile players and coaches, both domestic and international, contributing to its reputation as a top club in China.\\n\\nThe club is owned by the Sinobo Group, a real estate and entertainment conglomerate, which has invested in the team's development and infrastructure. Beijing Guoan continues to be a key player in the Chinese Super League, aiming to achieve further success both domestically and internationally.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 257, 'prompt_tokens': 27, 'total_tokens': 284, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d8864f8b6b', 'id': 'chatcmpl-BV7cLTidf3UTUbmAjoQBbET0b1JLT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--cb36b3a7-2ee1-4df4-936c-ec9b586de6f3-0' usage_metadata={'input_tokens': 27, 'output_tokens': 257, 'total_tokens': 284, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "#print(openai_api_key)\n",
    "llm = ChatOpenAI(api_key = openai_api_key,\n",
    "                model = \"gpt-4o\", \n",
    "                 temperature = 0, \n",
    "                 max_tokens = None,\n",
    "                timeout = None,\n",
    "                max_retries = 2)\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a Question answering machine\",\n",
    "    ),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"tell me something about Beijing Guoan soccer club?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(llm.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"data/Agent AI_Surveying the horizons of multimodal interaction.pdf\")\n",
    "document = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 80 chunks\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "texts = text_splitter.split_documents(document)\n",
    "print(f\"created {len(texts)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "persist_directory = \"chroma_db\"\n",
    "\n",
    "db = Chroma.from_documents(documents = texts, embedding = embeddings, persist_directory = persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.get()['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How LLM agent for robotics works', 'result': \"LLM (Large Language Model) agents for robotics work by leveraging advanced language processing capabilities to interpret instructions and decompose them into actionable steps for robots. These agents are integrated into various aspects of robotic systems to enhance their functionality and interaction with humans. Here's how they work in different contexts:\\n\\n1. **Multimodal Systems**: LLMs are used as encoders to process both linguistic instructions and visual cues, guiding robotic actions effectively. This integration allows robots to understand and act upon complex instructions that involve multiple types of input.\\n\\n2. **Task Planning and Skill Training**: LLMs help in high-level task planning by interpreting instructions and breaking them down into specific robot actions. They are also used in designing reward functions and generating data for policy learning, which aids in training efficient robot controllers.\\n\\n3. **On-site Optimization**: LLMs contribute to dynamically adapting and refining robotic skills by integrating task plans with real-time environmental data. They help in calculating the feasibility of actions and adjusting plans based on environmental conditions.\\n\\n4. **Conversation Agents**: LLMs enable robots to engage in natural, context-sensitive interactions with humans, processing and generating responses that mimic human conversation. This capability enhances human-robot communication by understanding human intent and generating meaningful gestures.\\n\\n5. **Navigation Agents**: LLMs assist in advanced navigation tasks, such as object navigation, where robots use object names instead of map coordinates. This requires visual grounding of object names in the environment, allowing robots to navigate more challenging environments.\\n\\nOverall, LLM agents for robotics enhance the ability of robots to understand and execute complex tasks, interact naturally with humans, and adapt to dynamic environments.\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = llm, chain_type= \"stuff\", retriever = db.as_retriever())\n",
    "\n",
    "response = qa.invoke(\"How LLM agent for robotics works\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent AI:\n",
      "Surveying the Horizons of Multimodal Interaction A PREPRINT\n",
      "While these elements—precise contact points and arm posture—are intuitive for humans, articulating them through\n",
      "language is challenging. Despite advances in internet-scale VLMs, capturing these nuanced indirect cues from scenes\n",
      "and translating them effectively into robotic skills remains a significant challenge. In response, the robotics community\n",
      "is increasingly focusing on collecting enhanced datasets(e.g., (Wang et al., 2023d; Padalkar et al., 2023)) or developing\n",
      "methodologies for direct skill acquisition from human demonstrations (Wake et al., 2021a). Frameworks including\n",
      "Learning-from-Demonstration and Imitation Learning are leading these developments, playing a crucial role in the\n",
      "optimization of physical skills.\n",
      "6.2.1 LLM/VLM Agent for Robotics.\n",
      "Recent research has demonstrated the potential of LLM/VLMs for robotic agents that involve interactions with humans\n",
      "in an environment. Research topics that aim to leverage latest LLM/VLM technologies include:\n",
      "Multimodal Systems: Recent research has been actively focusing on developing end-to-end systems that incorporate\n",
      "the latest LLM and VLM technologies as encoders for input information. Particularly, there is a significant trend\n",
      "towards modifying these foundation models to process multimodal information. (Jiang et al., 2022; Brohan et al., 2023,\n",
      "2022; Li et al., 2023d; Ahn et al., 2022b; Shah et al., 2023b; Li et al., 2023e). This adaptation aims to guide robotic\n",
      "actions based on both linguistic instructions and visual cues, thus achieving an effective embodiment.\n",
      "Task Planning and Skill Training: In contrast to end-to-end systems, Task And Motion Planning (TAMP) based\n",
      "systems first compute a high-level task plan and then achieve them with low-level robot control, known as skills.\n",
      "The advanced language processing abilities of LLMs have demonstrated the capability to interpret instructions and\n",
      "decompose them into robot action steps, greatly advancing task planning technologies (Ni et al., 2023; Li et al., 2023b;\n",
      "Parakh et al., 2023; Wake et al., 2023c). For skill training, several studies have explored the use of LLMs/VLMs\n",
      "for designing reward functions (Yu et al., 2023a; Katara et al., 2023; Ma et al., 2023), generating data to facilitate\n",
      "policy learning (Kumar et al., 2023; Du et al., 2023), or serving as part of a reward function (Sontakke et al., 2023).\n",
      "Together with training frameworks such as RL and IL, these efforts will contribute to the development of efficient robot\n",
      "controllers.\n",
      "On-site Optimization: Executing long task steps in robotics can be difficult due to unexpected and unpredictable\n",
      "environmental conditions. Therefore, a significant challenge in the field of robotics involves dynamically adapting and\n",
      "refining robotic skills by integrating task plans with real-time environmental data. For instance, (Ahn et al., 2022b)\n",
      "proposed an approach that calculates the feasibility of actions (i.e., affordance) from visual information and compares it\n",
      "with planned tasks. Additionally, there are approaches that focus on enabling LLMs to output the pre-conditions and\n",
      "post-conditions (e.g., states of objects and their interrelationships) of task steps to optimize their execution (Zhou et al.,\n",
      "2023c) and detect pre-condition errors for necessary revisions to the task plan (Raman et al., 2023). These strategies\n",
      "seek to achieve environment-grounded robot execution by integrating environmental information and adjusting the\n",
      "robot’s actions at the task plan or controller level.\n",
      "Conversation Agents: In creating conversational robots, LLMs can contribute to natural, context-sensitive interactions\n",
      "with humans (Ye et al., 2023a; Wake et al., 2023f). These models process and generate responses that mimic human\n",
      "conversation, allowing robots to participate in meaningful dialogues. Additionally, LLMs play a significant role in\n",
      "the estimation of conceptual (Hensel et al., 2023; Teshima et al., 2022) and emotional attributes (Zhao et al., 2023;\n",
      "Yang et al., 2023b; Wake et al., 2023d) of utterances. Those attributes facilitate the understanding of human intent and\n",
      "meaningful gesture generation, thus contributing to the naturalness and efficacy of human-robot communication.\n",
      "Navigation Agents: Robot navigation has a long history of research, focusing on core aspects such as map-based path\n",
      "planning and Simultaneous Localization and Mapping (SLAM) for creating environmental maps. These functionalities\n",
      "have become standard in widely used robot middleware like the Robot Operating System (ROS) (Guimarães et al.,\n",
      "2016).\n",
      "While classic navigation techniques remain prevalent in many robotics applications, they typically rely on static or\n",
      "pre-created maps. Recently, there has been an increased interest in advanced technologies that enable robots to navigate\n",
      "in more challenging environments, leveraging breakthroughs in fields like computer vision and natural language\n",
      "processing. One representative task is object navigation (Chaplot et al., 2020a; Batra et al., 2020; Gervet et al., 2023;\n",
      "Ramakrishnan et al., 2022; Zhang et al., 2021), where robots use object names for navigation instead of map coordinates,\n",
      "requiring the visual grounding of object names in the environment. Furthermore, recent attention has been given to\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "query = \"How LLM agent for robotics works?\"\n",
    "docs = db.similarity_search(query, k = 5)\n",
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
