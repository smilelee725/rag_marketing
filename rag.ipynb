{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"A Large Language Model (LLM) agent for robotics integrates natural language processing capabilities with robotic systems to enable more intuitive and flexible interactions between humans and robots. Here's a general overview of how such an agent might work:\\n\\n1. **Natural Language Understanding (NLU):** The LLM is used to process and understand human language inputs. This involves parsing commands, questions, or descriptions provided by a user. The model interprets the intent behind the language, extracting relevant information and context.\\n\\n2. **Task Planning and Decision Making:** Once the LLM understands the user's intent, it translates this into actionable tasks for the robot. This involves decision-making processes where the agent determines the sequence of actions required to achieve the desired outcome. This step may involve integrating with other AI models or systems that specialize in planning and decision-making.\\n\\n3. **Robotic Control and Execution:** The LLM agent interfaces with the robot's control systems to execute the planned tasks. This involves sending commands to the robot's actuators and sensors, ensuring that the robot performs the actions accurately and safely.\\n\\n4. **Feedback and Adaptation:** As the robot executes tasks, it may encounter new information or unexpected situations. The LLM agent can process feedback from the robot's sensors and adjust its actions accordingly. This adaptability is crucial for operating in dynamic environments.\\n\\n5. **Learning and Improvement:** Over time, the LLM agent can improve its performance through learning. This might involve updating its language understanding capabilities, refining task planning strategies, or enhancing its ability to adapt to new scenarios. Machine learning techniques, such as reinforcement learning, can be employed to facilitate this continuous improvement.\\n\\n6. **Integration with Other Systems:** An LLM agent for robotics often needs to integrate with other systems, such as computer vision for object recognition, mapping and localization systems for navigation, and databases for accessing additional information. This integration allows the agent to leverage a wide range of capabilities to perform complex tasks.\\n\\nOverall, an LLM agent for robotics aims to create a more seamless and natural interaction between humans and robots, enabling robots to understand and execute tasks based on human language inputs effectively.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 426, 'prompt_tokens': 25, 'total_tokens': 451, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9052abe5a7', 'id': 'chatcmpl-BV7Cp3nEYvcoYT0sR00leLSNPLvyq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--d69c1232-57f6-42ea-b8f7-8b3e9dc1adac-0' usage_metadata={'input_tokens': 25, 'output_tokens': 426, 'total_tokens': 451, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "#print(openai_api_key)\n",
    "llm = ChatOpenAI(api_key = openai_api_key,\n",
    "                model = \"gpt-4o\", \n",
    "                 temperature = 0, \n",
    "                 max_tokens = None,\n",
    "                timeout = None,\n",
    "                max_retries = 2)\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a Question answering machine\",\n",
    "    ),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"How LLM agent for robotics works?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(llm.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"data/Agent AI_Surveying the horizons of multimodal interaction.pdf\")\n",
    "document = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 80 chunks\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "texts = text_splitter.split_documents(document)\n",
    "print(f\"created {len(texts)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "persist_directory = \"chroma_db\"\n",
    "\n",
    "db = Chroma.from_documents(documents = texts, embedding = embeddings, persist_directory = persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.get()['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How LLM agent for robotics works', 'result': 'LLM agents for robotics work by leveraging the advanced language processing capabilities of large language models (LLMs) to interpret instructions and decompose them into actionable steps for robots. These agents are integrated into robotic systems to enhance various aspects of robot interaction and task execution. Here are some key components and functionalities of LLM agents in robotics:\\n\\n1. **Multimodal Systems**: LLMs are used in conjunction with visual language models (VLMs) to process both linguistic instructions and visual cues. This integration allows robots to understand and act upon complex instructions that involve both language and visual information.\\n\\n2. **Task Planning and Skill Training**: LLMs assist in high-level task planning by interpreting instructions and breaking them down into specific robot actions. They are also used in designing reward functions and generating data to facilitate policy learning, contributing to the development of efficient robot controllers.\\n\\n3. **On-site Optimization**: LLMs help in dynamically adapting and refining robotic skills by integrating task plans with real-time environmental data. They can calculate the feasibility of actions based on visual information and adjust task plans accordingly.\\n\\n4. **Conversation Agents**: LLMs enable robots to engage in natural, context-sensitive interactions with humans, processing and generating responses that mimic human conversation. This capability enhances human-robot communication by understanding human intent and generating meaningful gestures.\\n\\n5. **Navigation Agents**: LLMs contribute to advanced navigation technologies, allowing robots to navigate challenging environments using object names instead of map coordinates. This requires visual grounding of object names in the environment.\\n\\nOverall, LLM agents in robotics enhance the ability of robots to understand and execute complex tasks by integrating language understanding with visual and environmental data, enabling more intuitive and effective human-robot interactions.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = llm, chain_type= \"stuff\", retriever = db.as_retriever())\n",
    "\n",
    "response = qa.invoke(\"How LLM agent for robotics works\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent AI:\n",
      "Surveying the Horizons of Multimodal Interaction A PREPRINT\n",
      "While these elements—precise contact points and arm posture—are intuitive for humans, articulating them through\n",
      "language is challenging. Despite advances in internet-scale VLMs, capturing these nuanced indirect cues from scenes\n",
      "and translating them effectively into robotic skills remains a significant challenge. In response, the robotics community\n",
      "is increasingly focusing on collecting enhanced datasets(e.g., (Wang et al., 2023d; Padalkar et al., 2023)) or developing\n",
      "methodologies for direct skill acquisition from human demonstrations (Wake et al., 2021a). Frameworks including\n",
      "Learning-from-Demonstration and Imitation Learning are leading these developments, playing a crucial role in the\n",
      "optimization of physical skills.\n",
      "6.2.1 LLM/VLM Agent for Robotics.\n",
      "Recent research has demonstrated the potential of LLM/VLMs for robotic agents that involve interactions with humans\n",
      "in an environment. Research topics that aim to leverage latest LLM/VLM technologies include:\n",
      "Multimodal Systems: Recent research has been actively focusing on developing end-to-end systems that incorporate\n",
      "the latest LLM and VLM technologies as encoders for input information. Particularly, there is a significant trend\n",
      "towards modifying these foundation models to process multimodal information. (Jiang et al., 2022; Brohan et al., 2023,\n",
      "2022; Li et al., 2023d; Ahn et al., 2022b; Shah et al., 2023b; Li et al., 2023e). This adaptation aims to guide robotic\n",
      "actions based on both linguistic instructions and visual cues, thus achieving an effective embodiment.\n",
      "Task Planning and Skill Training: In contrast to end-to-end systems, Task And Motion Planning (TAMP) based\n",
      "systems first compute a high-level task plan and then achieve them with low-level robot control, known as skills.\n",
      "The advanced language processing abilities of LLMs have demonstrated the capability to interpret instructions and\n",
      "decompose them into robot action steps, greatly advancing task planning technologies (Ni et al., 2023; Li et al., 2023b;\n",
      "Parakh et al., 2023; Wake et al., 2023c). For skill training, several studies have explored the use of LLMs/VLMs\n",
      "for designing reward functions (Yu et al., 2023a; Katara et al., 2023; Ma et al., 2023), generating data to facilitate\n",
      "policy learning (Kumar et al., 2023; Du et al., 2023), or serving as part of a reward function (Sontakke et al., 2023).\n",
      "Together with training frameworks such as RL and IL, these efforts will contribute to the development of efficient robot\n",
      "controllers.\n",
      "On-site Optimization: Executing long task steps in robotics can be difficult due to unexpected and unpredictable\n",
      "environmental conditions. Therefore, a significant challenge in the field of robotics involves dynamically adapting and\n",
      "refining robotic skills by integrating task plans with real-time environmental data. For instance, (Ahn et al., 2022b)\n",
      "proposed an approach that calculates the feasibility of actions (i.e., affordance) from visual information and compares it\n",
      "with planned tasks. Additionally, there are approaches that focus on enabling LLMs to output the pre-conditions and\n",
      "post-conditions (e.g., states of objects and their interrelationships) of task steps to optimize their execution (Zhou et al.,\n",
      "2023c) and detect pre-condition errors for necessary revisions to the task plan (Raman et al., 2023). These strategies\n",
      "seek to achieve environment-grounded robot execution by integrating environmental information and adjusting the\n",
      "robot’s actions at the task plan or controller level.\n",
      "Conversation Agents: In creating conversational robots, LLMs can contribute to natural, context-sensitive interactions\n",
      "with humans (Ye et al., 2023a; Wake et al., 2023f). These models process and generate responses that mimic human\n",
      "conversation, allowing robots to participate in meaningful dialogues. Additionally, LLMs play a significant role in\n",
      "the estimation of conceptual (Hensel et al., 2023; Teshima et al., 2022) and emotional attributes (Zhao et al., 2023;\n",
      "Yang et al., 2023b; Wake et al., 2023d) of utterances. Those attributes facilitate the understanding of human intent and\n",
      "meaningful gesture generation, thus contributing to the naturalness and efficacy of human-robot communication.\n",
      "Navigation Agents: Robot navigation has a long history of research, focusing on core aspects such as map-based path\n",
      "planning and Simultaneous Localization and Mapping (SLAM) for creating environmental maps. These functionalities\n",
      "have become standard in widely used robot middleware like the Robot Operating System (ROS) (Guimarães et al.,\n",
      "2016).\n",
      "While classic navigation techniques remain prevalent in many robotics applications, they typically rely on static or\n",
      "pre-created maps. Recently, there has been an increased interest in advanced technologies that enable robots to navigate\n",
      "in more challenging environments, leveraging breakthroughs in fields like computer vision and natural language\n",
      "processing. One representative task is object navigation (Chaplot et al., 2020a; Batra et al., 2020; Gervet et al., 2023;\n",
      "Ramakrishnan et al., 2022; Zhang et al., 2021), where robots use object names for navigation instead of map coordinates,\n",
      "requiring the visual grounding of object names in the environment. Furthermore, recent attention has been given to\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "query = \"How LLM agent for robotics works?\"\n",
    "docs = db.similarity_search(query, k = 5)\n",
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
